{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "'exec%matplotlib inline'\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob , natsort\n",
    "\n",
    "\n",
    "### -------------------------- iternet----------------------\n",
    "from keras import losses\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Input, MaxPooling2D\n",
    "from keras.layers import concatenate, Conv2D, Conv2DTranspose, Dropout, ReLU, BatchNormalization, Activation\n",
    "from keras.layers.merge import add, multiply\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam,  RMSprop, Adadelta\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop, Adam, Adadelta\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "#env: PyHIST, image size : 300\n",
    "# import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflowgpu==1.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keras.__version__)\n",
    "print('python: ', sys.version )\n",
    "print('tensorflow: ', tf.__version__ )\n",
    "from keras import backend as K\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "# Need to install : \n",
    "# $ pip install openpyxl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../AI_competition/'\n",
    "folders = ['csv', '먹이생물']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = os.listdir(os.path.join(directory,'csv'))\n",
    "print(days[1])\n",
    "address = ['고성', '일해']\n",
    "csv_file = ['*-0020000.csv','*-0010000.csv'] # '-0020000.csv' = '고성', '-0010000.csv' = 일해'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x[0] for x in os.walk(directory)]\n",
    "goseong_code = []\n",
    "ilhe_code = []\n",
    "goseong_ntu = []\n",
    "ilhe_ntu = []\n",
    "day_cutoff = 1\n",
    "for i_day in days: \n",
    "    # '고성'\n",
    "    for file in glob.glob(os.path.join(directory, folders[0],i_day,csv_file[0])):\n",
    "        # print(file)\n",
    "        data  = pd.read_csv(file)\n",
    "        data['ID CODE'] =  folders[1]+'/'+i_day + '/'+address[0]+'/'+data['ID CODE'] +'.jpg'\n",
    "        idcode = data['ID CODE'].to_list()\n",
    "        ntu = data['NTU'].to_numpy()\n",
    "        goseong_code.extend(idcode)\n",
    "        goseong_ntu.extend(ntu)\n",
    "    # 일해\n",
    "    for file1 in glob.glob(os.path.join(directory, folders[0],i_day,csv_file[1])):\n",
    "        data1  = pd.read_csv(file1)\n",
    "        data1['ID CODE'] =   folders[1]+'/'+i_day + '/'+address[1]+'/' +data1['ID CODE'] +'.jpg'\n",
    "        idcode1 = data1['ID CODE'].to_list()\n",
    "        ntu1 = data1['NTU'].to_numpy()\n",
    "        ilhe_code.extend(idcode1)\n",
    "        \n",
    "        ilhe_ntu.extend(ntu1)\n",
    "    if day_cutoff==1:\n",
    "        break\n",
    "    day_cutoff +=1\n",
    "        \n",
    "print(len(goseong_code))\n",
    "print(len(goseong_ntu))\n",
    "\n",
    "print(len(ilhe_code))\n",
    "print(len(ilhe_ntu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goseong_code_1 = []\n",
    "goseong_ntu_1 = []\n",
    "for i in range(len(goseong_code)-1):\n",
    "\n",
    "    if os.path.exists(os.path.join(directory,goseong_code[i])):\n",
    "        goseong_code_1.append(goseong_code[i])\n",
    "        goseong_ntu_1.append(goseong_ntu[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goseong_code = goseong_code_1\n",
    "goseong_ntu = goseong_ntu_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goseongObj = zip(goseong_code, goseong_ntu)\n",
    "dict_goseong = dict(goseongObj)\n",
    "zipilhe = zip(ilhe_code,ilhe_ntu)\n",
    "dict_ilhe= dict(zipilhe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goseong_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_day in days: \n",
    "    # '고성'\n",
    "    for file in goseong_code:\n",
    "        # /media/tuongvi/새 볼륨/AI_competition/먹이생물/10월01일/고성/\n",
    "        img_gos = os.path.join(directory,file)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(img_gos,0)\n",
    "print(image.shape)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ilhe['먹이생물/10월01일/일해/2-1-1-2-2-1001-0110011.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class DataGen(keras.utils.Sequence):\n",
    "    def __init__(self,ids, path, code, dict_file_survival, batch_size=1, image_size=256, augmentations = None, Train=False):\n",
    "        self.ids = ids\n",
    "        self.path = path\n",
    "        self.code = code\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augmentations\n",
    "        self.train = Train\n",
    "        self.dict_file_survival = dict_file_survival\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __load__(self, id_name):\n",
    "        # Init\n",
    "        id_name = self.code[id_name]\n",
    "        \n",
    "        ## Reading images\n",
    "        \n",
    "        image = cv2.imread(os.path.join(self.path,id_name),0) \n",
    "        image = cv2.resize(image,(self.image_size, self.image_size))\n",
    "        image = np.expand_dims(image, axis=-1)\n",
    "\n",
    "        label = np.float32(self.dict_file_survival[str(id_name)])\n",
    "        \n",
    "        if self.augment :\n",
    "            data = {\n",
    "                \"image\": np.array(image),\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "        return image, label\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if(index + 1) * self.batch_size >len(self.ids):\n",
    "            batch_size_tmp = len(self.ids) - index * self.batch_size\n",
    "        else:\n",
    "            batch_size_tmp = self.batch_size\n",
    "            \n",
    "        \n",
    "        files_batch = self.ids[index * batch_size_tmp : (index+1)*batch_size_tmp]\n",
    "        \n",
    "        image = []\n",
    "        lable = []\n",
    "        for id_name in files_batch:\n",
    "            _img, _lable = self.__load__(id_name)\n",
    "            image.append(_img)\n",
    "            lable.append(_lable)\n",
    "        \n",
    "        image = np.array(image)\n",
    "        lable = np.array(lable, dtype=np.float32)\n",
    "        \n",
    "        ## Normalization\n",
    "        image = image / 255.0\n",
    "\n",
    "\n",
    "        return image,lable\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.ids)/float(self.batch_size)))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Conv2D, MaxPooling2D, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom:\n",
    "def cnn_model(pixels,kn=3):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(4, kernel_size=(kn, kn), input_shape=(pixels, pixels,1), padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.7))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Conv2D(2, kernel_size=(kn, kn), padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.7))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Conv2D(2, kernel_size=(kn*2, kn*2), padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.7))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_gen, train_steps, fold_no, pixels, knp,valid_gen=None, valid_steps=None, weights=None):\n",
    "    kn = knp\n",
    "    my_model = cnn_model(pixels,kn) # ours\n",
    "    weights_name = 'Fold_'+fold_no+'_kernelsize_'+str(kn)+'.hdf5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(weights_name, monitor='mae', save_best_only=True)\n",
    "\n",
    "        \n",
    "    my_model.compile(loss='mean_absolute_error', optimizer= keras.optimizers.Adam(lr=1e-3, decay=1e-3 / 200), metrics=['mae']) # mean_squared_logarithmic_error # mean_absolute_error # mean_squared_error\n",
    "\n",
    "    history = my_model.fit_generator(train_gen, steps_per_epoch=train_steps, shuffle=True, epochs=30,  callbacks=[checkpoint])\n",
    "        \n",
    "    scoreSeg = my_model.evaluate_generator(valid_gen, valid_steps)\n",
    "    print(\"Accuracy = \",scoreSeg[1])\n",
    "    print(\"Done with top layers training.\")\n",
    "    return my_model, history, weights_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "train_path = directory\n",
    "fo =1\n",
    "for train_ids, test_ids in kf.split(goseong_code):\n",
    "    print(len(train_ids), len(test_ids))\n",
    "    # print(\"%s %s\" % (train_ids, test_ids))\n",
    "    img_size = 128\n",
    "    batch_size=16\n",
    "    train_gen = DataGen(train_ids, train_path, goseong_code, dict_goseong, batch_size=batch_size,image_size=img_size)\n",
    "    train_steps = int(len(train_ids)/batch_size)\n",
    "    \n",
    "    test_gen = DataGen(test_ids, train_path, goseong_code, dict_goseong, batch_size=batch_size,image_size=img_size)\n",
    "    test_steps = int(len(test_ids)/batch_size)\n",
    "    \n",
    "    \n",
    "    my_model, history, weights_name = train(train_gen, train_steps,str(fo), img_size,knp= 5,valid_gen= test_gen,valid_steps=test_steps)\n",
    "    fo +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
